#!/usr/bin/env python3
"""
çµæ´»çš„è°ƒåº¦æµ‹è¯•æ¡†æ¶ - æ”¯æŒå¤šç§é…ç½®åœºæ™¯
"""

import pytest
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from NNScheduler.core.resource_queue import ResourceQueueManager
from NNScheduler.core.schedule_tracer import ScheduleTracer
from NNScheduler.core.enhanced_launcher import EnhancedTaskLauncher
from NNScheduler.core.executor import ScheduleExecutor
from NNScheduler.core.evaluator import PerformanceEvaluator
from NNScheduler.core.artifacts import ensure_artifact_path, resolve_artifact_path
from NNScheduler.core.enums import ResourceType
from NNScheduler.viz.schedule_visualizer import ScheduleVisualizer

# å¯¼å…¥é…ç½®ç±»
try:
    from NNScheduler.core.scheduling_config import SchedulingConfig
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼
    print("Warning: Could not import SchedulingConfig from NNScheduler.core.scheduling_config")
    SchedulingConfig = None


@dataclass
class SchedulingTestResult:
    """æµ‹è¯•ç»“æœ"""
    config: 'SchedulingConfig'
    stats: Dict
    metrics: 'PerformanceMetrics'
    utilization: Dict[str, float]
    system_utilization: float
    tracer: ScheduleTracer
    
    def summary(self) -> str:
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        lines = [
            f"é…ç½®: {self.config.get_resource_summary()}",
            f"å®Œæˆå®ä¾‹: {self.stats['completed_instances']}",
            f"Systemåˆ©ç”¨ç‡: {self.system_utilization:.1f}%",
            f"FPSæ»¡è¶³ç‡: {self.metrics.fps_satisfaction_rate:.1f}%"
        ]
        return "\n".join(lines)


class SchedulingTestFramework:
    """è°ƒåº¦æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, tasks: List):
        """
        åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
        """
        self.tasks = tasks
        self.results: Dict[str, SchedulingTestResult] = {}
    
    def calculate_theory_demand(self, tasks: List, config: SchedulingConfig) -> Dict:
        """
        è®¡ç®—ç†è®ºèµ„æºéœ€æ±‚
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            config: è°ƒåº¦é…ç½®
            
        Returns:
            ç†è®ºéœ€æ±‚åˆ†æç»“æœ
        """
        # è·å–å¹³å‡å¸¦å®½
        npu_bandwidth = config.get_npu_bandwidth()
        dsp_bandwidth = config.get_dsp_bandwidth()
        time_window = config.analysis_window
        
        npu_total_time = 0.0
        dsp_total_time = 0.0
        
        for task in tasks:
            # è®¡ç®—ä»»åŠ¡åœ¨æ—¶é—´çª—å£å†…éœ€è¦æ‰§è¡Œçš„æ¬¡æ•°
            instances_needed = task.fps_requirement * (time_window / 1000.0)
            
            # åº”ç”¨åˆ†æ®µç­–ç•¥è·å–å®é™…æ‰§è¡Œçš„æ®µ
            segments = task.apply_segmentation()
            if not segments:
                segments = task.segments
            
            # è®¡ç®—æ¯ä¸ªæ®µçš„æ‰§è¡Œæ—¶é—´
            for seg in segments:
                if seg.resource_type.value == "NPU":
                    duration = seg.get_duration(npu_bandwidth)
                    npu_total_time += duration * instances_needed
                elif seg.resource_type.value == "DSP":
                    duration = seg.get_duration(dsp_bandwidth)
                    dsp_total_time += duration * instances_needed
        
        # è®¡ç®—åˆ©ç”¨ç‡
        npu_utilization = (npu_total_time / time_window) * 100
        dsp_utilization = (dsp_total_time / time_window) * 100
        
        return {
            'npu_demand_ms': npu_total_time,
            'dsp_demand_ms': dsp_total_time,
            'npu_utilization': npu_utilization,
            'dsp_utilization': dsp_utilization,
            'feasible': npu_utilization <= 100 and dsp_utilization <= 100
        }
    
    def run_test(self, config: SchedulingConfig, verbose: bool = True) -> SchedulingTestResult:
        """
        è¿è¡Œå•ä¸ªæµ‹è¯•
        
        Args:
            config: è°ƒåº¦é…ç½®
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        if verbose:
            config.print_config()
        
        # 1. åˆ›å»ºèµ„æº
        queue_manager = ResourceQueueManager()
        for resource in config.resources:
            queue_manager.add_resource(
                resource.resource_id,
                resource.resource_type,
                resource.bandwidth
            )
        
        # 2. åˆ›å»ºè¿½è¸ªå™¨å’Œå¯åŠ¨å™¨
        tracer = ScheduleTracer(queue_manager)
        launcher = EnhancedTaskLauncher(queue_manager, tracer)
        
        # 3. æ³¨å†Œä»»åŠ¡
        for task in self.tasks:
            launcher.register_task(task)
        
        # 4. åˆ›å»ºå’Œæ‰§è¡Œè®¡åˆ’
        plan = launcher.create_launch_plan(
            config.simulation_duration,
            config.launch_strategy
        )
        
        executor = ScheduleExecutor(queue_manager, tracer, launcher.tasks)
        stats = executor.execute_plan(
            plan,
            config.simulation_duration,
            segment_mode=config.segment_mode
        )
        
        # 5. è¯„ä¼°æ€§èƒ½
        evaluator = PerformanceEvaluator(tracer, launcher.tasks, queue_manager)
        metrics = evaluator.evaluate(config.simulation_duration, plan.events)
        
        # 6. è®¡ç®—åˆ©ç”¨ç‡
        resource_utilization = tracer.get_resource_utilization(
            time_window=config.analysis_window
        )
        system_util = self._calculate_system_utilization(
            tracer, config.analysis_window
        )
        
        # 7. åˆ›å»ºç»“æœ
        result = SchedulingTestResult(
            config=config,
            stats=stats,
            metrics=metrics,
            utilization=resource_utilization,
            system_utilization=system_util,
            tracer=tracer
        )
        
        # 8. ä¿å­˜ç»“æœ
        self.results[config.scenario_name] = result
        
        if verbose:
            self._print_test_result(result)
        
        return result
    
    def run_comparison_tests(self, configs: List[SchedulingConfig]) -> Dict[str, SchedulingTestResult]:
        """
        è¿è¡Œå¤šä¸ªé…ç½®çš„å¯¹æ¯”æµ‹è¯•
        
        Args:
            configs: é…ç½®åˆ—è¡¨
            
        Returns:
            ç»“æœå­—å…¸
        """
        print("\nğŸ”¬ å¼€å§‹å¯¹æ¯”æµ‹è¯•")
        print("="*80)
        
        for config in configs:
            print(f"\nâ–¶ æµ‹è¯•åœºæ™¯: {config.scenario_name}")
            self.run_test(config, verbose=False)
            print(f"  [OK] å®Œæˆ")
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        self._print_comparison_results()
        
        return self.results
    
    def generate_visualizations(self, output_dir: str = "results"):
        """
        ä¸ºæ‰€æœ‰æµ‹è¯•ç»“æœç”Ÿæˆå¯è§†åŒ–
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        output_dir_path = resolve_artifact_path(output_dir)
        output_dir_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\n[ANALYSIS] ç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶åˆ° {output_dir_path}/")

        for name, result in self.results.items():
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_name = name.replace(" ", "_").replace("Ã—", "x").replace("+", "_")
            
            visualizer = ScheduleVisualizer(result.tracer)
            
            # ç”ŸæˆPNG
            png_file = output_dir_path / f"{safe_name}.png"
            visualizer.plot_resource_timeline(png_file)
            
            # ç”ŸæˆChrome Trace
            json_file = output_dir_path / f"{safe_name}.json"
            visualizer.export_chrome_tracing(json_file)
            
            print(f"  [OK] {name}: {safe_name}.png, {safe_name}.json")
    
    def export_comparison_report(self, filename: str = "comparison_report.txt"):
        """å¯¼å‡ºå¯¹æ¯”æŠ¥å‘Š"""
        output_path = ensure_artifact_path(filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("è°ƒåº¦ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            # å†™å…¥è¯¦ç»†ç»“æœ
            for name, result in self.results.items():
                f.write(f"\n{name}\n")
                f.write("-"*40 + "\n")
                f.write(result.summary() + "\n")
                
                # èµ„æºåˆ©ç”¨ç‡è¯¦æƒ…
                f.write("\nèµ„æºåˆ©ç”¨ç‡:\n")
                for res_id, util in sorted(result.utilization.items()):
                    f.write(f"  {res_id}: {util:.1f}%\n")
        
        print(f"\nğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    def _calculate_system_utilization(self, tracer, window_size):
        """è®¡ç®—ç³»ç»Ÿåˆ©ç”¨ç‡"""
        busy_intervals = []
        
        for exec in tracer.executions:
            if exec.start_time is not None and exec.end_time is not None:
                busy_intervals.append((exec.start_time, exec.end_time))
        
        if not busy_intervals:
            return 0.0
        
        # åˆå¹¶é‡å çš„æ—¶é—´æ®µ
        busy_intervals.sort()
        merged_intervals = []
        
        for start, end in busy_intervals:
            if merged_intervals and start <= merged_intervals[-1][1]:
                merged_intervals[-1] = (merged_intervals[-1][0], max(merged_intervals[-1][1], end))
            else:
                merged_intervals.append((start, end))
        
        total_busy_time = sum(end - start for start, end in merged_intervals)
        return (total_busy_time / window_size) * 100.0
    
    def _print_test_result(self, result: SchedulingTestResult):
        """æ‰“å°å•ä¸ªæµ‹è¯•ç»“æœ"""
        print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
        print(f"  å®Œæˆå®ä¾‹: {result.stats['completed_instances']}")
        print(f"  æ‰§è¡Œæ®µæ•°: {result.stats['total_segments_executed']}")
        print(f"  Systemåˆ©ç”¨ç‡: {result.system_utilization:.1f}%")
        print(f"  å¹³å‡ç­‰å¾…æ—¶é—´: {result.metrics.avg_wait_time:.2f}ms")
        print(f"  FPSæ»¡è¶³ç‡: {result.metrics.fps_satisfaction_rate*100:.1f}%")
        
        # æ‰“å°ä¸»è¦èµ„æºåˆ©ç”¨ç‡
        npu_utils = [(k, v) for k, v in result.utilization.items() if 'NPU' in k]
        dsp_utils = [(k, v) for k, v in result.utilization.items() if 'DSP' in k]
        
        if npu_utils:
            print(f"\n  NPUåˆ©ç”¨ç‡:")
            for res_id, util in sorted(npu_utils):
                print(f"    {res_id}: {util:.1f}%")
        
        if dsp_utils:
            print(f"\n  DSPåˆ©ç”¨ç‡:")
            for res_id, util in sorted(dsp_utils):
                print(f"    {res_id}: {util:.1f}%")
    
    def _print_comparison_results(self):
        """æ‰“å°å¯¹æ¯”ç»“æœè¡¨æ ¼"""
        if not self.results:
            return
        
        print("\n\n[ANALYSIS] å¯¹æ¯”ç»“æœæ±‡æ€»")
        print("="*100)
        
        # è¡¨å¤´
        headers = ["é…ç½®", "å®Œæˆå®ä¾‹", "Systemåˆ©ç”¨ç‡", "å¹³å‡NPUåˆ©ç”¨ç‡", "å¹³å‡DSPåˆ©ç”¨ç‡", "FPSæ»¡è¶³ç‡"]
        col_widths = [25, 10, 15, 15, 15, 12]
        
        # æ‰“å°è¡¨å¤´
        header_line = ""
        for header, width in zip(headers, col_widths):
            header_line += f"{header:<{width}}"
        print(header_line)
        print("-"*100)
        
        # æ‰“å°æ•°æ®è¡Œ
        for name, result in self.results.items():
            # è®¡ç®—å¹³å‡åˆ©ç”¨ç‡
            npu_utils = [v for k, v in result.utilization.items() if 'NPU' in k]
            dsp_utils = [v for k, v in result.utilization.items() if 'DSP' in k]
            
            avg_npu = sum(npu_utils) / len(npu_utils) if npu_utils else 0
            avg_dsp = sum(dsp_utils) / len(dsp_utils) if dsp_utils else 0
            
            row = [
                name[:24],  # æˆªæ–­è¿‡é•¿çš„åç§°
                str(result.stats['completed_instances']),
                f"{result.system_utilization:.1f}%",
                f"{avg_npu:.1f}%",
                f"{avg_dsp:.1f}%",
                f"{result.metrics.fps_satisfaction_rate:.1f}%"
            ]
            
            row_line = ""
            for cell, width in zip(row, col_widths):
                row_line += f"{cell:<{width}}"
            print(row_line)


def demo_framework_usage():
    """æ¼”ç¤ºæµ‹è¯•æ¡†æ¶çš„ç”¨æ³•"""
    print("=" * 80)
    print("[DEMO] çµæ´»è°ƒåº¦æµ‹è¯•æ¡†æ¶æ¼”ç¤º")
    print("=" * 80)
    
    print("[INFO] è¿™æ˜¯ä¸€ä¸ªç”¨äºè°ƒåº¦ç³»ç»Ÿæµ‹è¯•çš„çµæ´»æ¡†æ¶")
    print("\næ¡†æ¶åŠŸèƒ½:")
    print("  - æ”¯æŒå¤šç§èµ„æºé…ç½®çš„å¹¶è¡Œæµ‹è¯•")
    print("  - è‡ªåŠ¨æ”¶é›†å’Œæ¯”è¾ƒæ€§èƒ½æŒ‡æ ‡")
    print("  - ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šå’Œå¯è§†åŒ–")
    print("  - å¯æ‰©å±•çš„æµ‹è¯•é…ç½®ç³»ç»Ÿ")
    
    print("\nä¸»è¦ç±»å’Œæ–¹æ³•:")
    print("  - TestResult: å­˜å‚¨å•æ¬¡æµ‹è¯•çš„ç»“æœæ•°æ®")
    print("  - SchedulingTestFramework: ä¸»æ¡†æ¶ç±»")
    print("    - run_test(): è¿è¡Œå•ä¸ªé…ç½®æµ‹è¯•")
    print("    - print_comparison(): æ‰“å°ç»“æœå¯¹æ¯”è¡¨")
    print("    - generate_report(): ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("  ```python")
    print("  from flexible_test_framework import SchedulingTestFramework")
    print("  from scenario.real_task import create_real_tasks")
    print("  ")
    print("  # åˆ›å»ºæµ‹è¯•ä»»åŠ¡")
    print("  tasks = create_real_tasks()")
    print("  ")
    print("  # åˆ›å»ºæ¡†æ¶")
    print("  framework = SchedulingTestFramework(tasks)")
    print("  ")
    print("  # è¿è¡Œæµ‹è¯•")
    print("  result = framework.run_test(your_config)")
    print("  ```")
    
    print("\n[TIP] æ­¤æ¡†æ¶è®¾è®¡ç”¨äº:")
    print("  - å¯¹æ¯”ä¸åŒèµ„æºé…ç½®çš„æ€§èƒ½")
    print("  - è‡ªåŠ¨åŒ–æµ‹è¯•å¤šç§è°ƒåº¦ç­–ç•¥")
    print("  - ç”Ÿæˆç³»ç»Ÿæ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
    print("  - æ”¯æŒå¤§è§„æ¨¡è°ƒåº¦å®éªŒ")
    
    print("\n[OK] æ¡†æ¶è¯´æ˜å®Œæˆ")
    print("[NOTE] è¦å®é™…è¿è¡Œæµ‹è¯•ï¼Œè¯·ç¡®ä¿é…ç½®ç±»å’Œä¾èµ–é¡¹æ­£ç¡®è®¾ç½®")


if __name__ == "__main__":
    demo_framework_usage()
