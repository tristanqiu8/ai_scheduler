#!/usr/bin/env python3
"""
æ€§èƒ½è¯„ä¼°å™¨ - å…¨é¢è¯„ä¼°è°ƒåº¦å™¨çš„æ‰§è¡Œæ€§èƒ½
åŒ…æ‹¬FPSã€å»¶è¿Ÿã€èµ„æºåˆ©ç”¨ç‡ã€ç©ºé—²æ—¶é—´ç­‰å…³é”®æŒ‡æ ‡
"""

from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict
import json

from .enums import TaskPriority, ResourceType
from .task import NNTask
from .schedule_tracer import ScheduleTracer, TaskExecution
from .resource_queue import ResourceQueueManager


@dataclass
class TaskPerformanceMetrics:
    """å•ä¸ªä»»åŠ¡çš„æ€§èƒ½æŒ‡æ ‡"""
    task_id: str
    task_name: str
    priority: TaskPriority
    fps_requirement: float
    latency_requirement: float
    
    # æ‰§è¡Œç»Ÿè®¡
    execution_count: int = 0
    instance_count: int = 0
    
    # FPSç›¸å…³
    achieved_fps: float = 0.0
    fps_satisfaction: bool = False
    fps_achievement_rate: float = 0.0  # è¾¾æˆç‡ç™¾åˆ†æ¯”
    
    # å»¶è¿Ÿç›¸å…³
    wait_times: List[float] = field(default_factory=list)  # å‘å°„åˆ°å¼€å§‹è°ƒåº¦çš„æ—¶å»¶
    latencies: List[float] = field(default_factory=list)   # å‘å°„åˆ°å®Œæˆçš„æ€»å»¶è¿Ÿ
    avg_wait_time: float = 0.0
    avg_latency: float = 0.0
    max_latency: float = 0.0
    latency_violations: int = 0
    latency_satisfaction_rate: float = 0.0
    
    # æ‰§è¡Œæ—¶é—´ç»Ÿè®¡
    execution_times: List[float] = field(default_factory=list)
    avg_execution_time: float = 0.0
    total_execution_time: float = 0.0


@dataclass
class ResourceUtilizationMetrics:
    """èµ„æºåˆ©ç”¨ç‡æŒ‡æ ‡"""
    resource_id: str
    resource_type: ResourceType
    capacity: float
    
    # æ—¶é—´ç»Ÿè®¡
    busy_time: float = 0.0
    idle_time: float = 0.0
    total_time: float = 0.0
    
    # åˆ©ç”¨ç‡
    utilization_rate: float = 0.0  # busy_time / total_time
    
    # ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡
    task_executions: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    segment_executions: int = 0


@dataclass
class OverallPerformanceMetrics:
    """æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
    # æ—¶é—´çª—å£
    time_window: float
    actual_execution_time: float  # å®é™…æ‰§è¡Œæ—¶é—´ï¼ˆæœ€åä¸€ä¸ªä»»åŠ¡å®Œæˆæ—¶é—´ï¼‰
    idle_time: float  # æ•´ä½“è¿è¡Œå®Œæˆåˆ°æ•´ä½“çª—å£ç»“æŸçš„idleæ—¶é—´
    idle_time_ratio: float  # idleæ—¶é—´å æ¯”
    
    # FPSç»Ÿè®¡
    total_fps_requirement: float
    achieved_total_fps: float
    fps_satisfaction_rate: float  # æ»¡è¶³FPSè¦æ±‚çš„ä»»åŠ¡æ¯”ä¾‹
    avg_fps_achievement_rate: float  # å¹³å‡FPSè¾¾æˆç‡
    
    # å»¶è¿Ÿç»Ÿè®¡
    avg_wait_time: float
    avg_latency: float
    max_latency: float
    latency_violation_rate: float
    
    # èµ„æºåˆ©ç”¨ç‡
    avg_npu_utilization: float
    avg_dsp_utilization: float
    overall_resource_utilization: float
    resource_balance_score: float  # èµ„æºè´Ÿè½½å‡è¡¡åº¦(0-1)
    
    # ä»»åŠ¡å®Œæˆæƒ…å†µ
    total_tasks: int
    completed_tasks: int
    completion_rate: float
    total_segments: int
    completed_segments: int


class PerformanceEvaluator:
    """æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self, tracer: ScheduleTracer, tasks: Dict[str, NNTask], 
                 queue_manager: ResourceQueueManager):
        self.tracer = tracer
        self.tasks = tasks
        self.queue_manager = queue_manager
        self.time_window = 0.0
        
        # è¯„ä¼°ç»“æœ
        self.task_metrics: Dict[str, TaskPerformanceMetrics] = {}
        self.resource_metrics: Dict[str, ResourceUtilizationMetrics] = {}
        self.overall_metrics: Optional[OverallPerformanceMetrics] = None
        
    def evaluate(self, time_window: float, launch_events: List = None) -> OverallPerformanceMetrics:
        """
        æ‰§è¡Œå…¨é¢çš„æ€§èƒ½è¯„ä¼°
        
        Args:
            time_window: æ—¶é—´çª—å£
            launch_events: å‘å°„äº‹ä»¶åˆ—è¡¨(ç”¨äºè®¡ç®—ç­‰å¾…æ—¶é—´)
            
        Returns:
            æ•´ä½“æ€§èƒ½æŒ‡æ ‡
        """
        self.time_window = time_window
        
        # 1. è¯„ä¼°ä»»åŠ¡æ€§èƒ½
        self._evaluate_task_performance(launch_events)
        
        # 2. è¯„ä¼°èµ„æºåˆ©ç”¨ç‡
        self._evaluate_resource_utilization()
        
        # 3. è®¡ç®—æ•´ä½“æŒ‡æ ‡
        self._calculate_overall_metrics()
        
        return self.overall_metrics
        
    def _evaluate_task_performance(self, launch_events: List = None):
        """è¯„ä¼°æ¯ä¸ªä»»åŠ¡çš„æ€§èƒ½"""
        # åˆå§‹åŒ–ä»»åŠ¡æŒ‡æ ‡
        for task_id, task in self.tasks.items():
            self.task_metrics[task_id] = TaskPerformanceMetrics(
                task_id=task_id,
                task_name=task.name,
                priority=task.priority,
                fps_requirement=task.fps_requirement,
                latency_requirement=task.latency_requirement
            )
        
        # æ„å»ºå‘å°„æ—¶é—´æ˜ å°„
        launch_times = defaultdict(list)  # task_id -> [(instance_id, launch_time)]
        if launch_events:
            for event in launch_events:
                # å°è¯•è·å–æ—¶é—´å±æ€§ï¼ˆå…¼å®¹ä¸åŒçš„å±æ€§åï¼‰
                launch_time = None
                if hasattr(event, 'time'):
                    launch_time = event.time
                elif hasattr(event, 'launch_time'):
                    launch_time = event.launch_time
                
                if launch_time is not None and hasattr(event, 'task_id'):
                    instance_id = getattr(event, 'instance_id', 0)
                    launch_times[event.task_id].append((instance_id, launch_time))
        
        # åˆ†ææ‰§è¡Œå†å²
        task_instances = defaultdict(lambda: defaultdict(list))  # task_id -> instance -> executions
        
        for execution in self.tracer.executions:
            # è§£æä»»åŠ¡IDå’Œå®ä¾‹å·
            if '#' in execution.task_id:
                base_task_id, instance_info = execution.task_id.split('#', 1)
                if '_' in instance_info:
                    instance_num = int(instance_info.split('_')[0])
                else:
                    instance_num = int(instance_info)
            else:
                base_task_id = execution.task_id
                instance_num = 0
            
            if base_task_id in self.task_metrics:
                task_instances[base_task_id][instance_num].append(execution)
        
        # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æŒ‡æ ‡
        for task_id, instances in task_instances.items():
            metrics = self.task_metrics[task_id]
            metrics.instance_count = len(instances)
            
            # å¤„ç†æ¯ä¸ªå®ä¾‹
            for instance_num, executions in instances.items():
                if not executions:
                    continue
                
                # æ‰¾åˆ°è¯¥å®ä¾‹çš„ç¬¬ä¸€æ¬¡å’Œæœ€åä¸€æ¬¡æ‰§è¡Œ
                first_exec = min(executions, key=lambda e: e.start_time)
                last_exec = max(executions, key=lambda e: e.end_time)
                
                # æŸ¥æ‰¾å¯¹åº”çš„å‘å°„æ—¶é—´
                launch_time = None
                if task_id in launch_times:
                    # æŸ¥æ‰¾åŒ¹é…çš„å®ä¾‹å‘å°„æ—¶é—´
                    for inst_id, l_time in launch_times[task_id]:
                        if inst_id == instance_num:
                            launch_time = l_time
                            break
                
                # è®¡ç®—å»¶è¿Ÿ
                if launch_time is not None:
                    # ç­‰å¾…æ—¶é—´ï¼šå‘å°„åˆ°é¦–æ¬¡æ‰§è¡Œ
                    wait_time = first_exec.start_time - launch_time
                    metrics.wait_times.append(wait_time)
                    
                    # æ€»å»¶è¿Ÿï¼šå‘å°„åˆ°å®Œæˆ
                    total_latency = last_exec.end_time - launch_time
                    metrics.latencies.append(total_latency)
                    
                    # æ£€æŸ¥å»¶è¿Ÿè¿è§„
                    if total_latency > metrics.latency_requirement:
                        metrics.latency_violations += 1
                
                # ç´¯è®¡æ‰§è¡Œæ—¶é—´
                for exec in executions:
                    metrics.execution_times.append(exec.duration)
                    metrics.total_execution_time += exec.duration
                    metrics.execution_count += 1
            
            # è®¡ç®—å¹³å‡å€¼
            if metrics.wait_times:
                metrics.avg_wait_time = sum(metrics.wait_times) / len(metrics.wait_times)
            
            if metrics.latencies:
                metrics.avg_latency = sum(metrics.latencies) / len(metrics.latencies)
                metrics.max_latency = max(metrics.latencies)
                metrics.latency_satisfaction_rate = 1.0 - (metrics.latency_violations / len(metrics.latencies))
            
            if metrics.execution_times:
                metrics.avg_execution_time = sum(metrics.execution_times) / len(metrics.execution_times)
            
            # è®¡ç®—FPS
            if self.time_window > 0:
                metrics.achieved_fps = (metrics.instance_count * 1000.0) / self.time_window
                metrics.fps_achievement_rate = min(100.0, (metrics.achieved_fps / metrics.fps_requirement) * 100.0)
                metrics.fps_satisfaction = metrics.achieved_fps >= metrics.fps_requirement
    def _evaluate_resource_utilization(self):
        """è¯„ä¼°èµ„æºåˆ©ç”¨ç‡"""
        # ç›´æ¥ä» resource_queues è·å–æ‰€æœ‰èµ„æº
        for res_id, queue in self.queue_manager.resource_queues.items():
            self.resource_metrics[res_id] = ResourceUtilizationMetrics(
                resource_id=res_id,
                resource_type=queue.resource_type,
                capacity=queue.bandwidth,
                total_time=self.time_window
            )
        
        # åˆ†ææ¯ä¸ªèµ„æºçš„æ‰§è¡Œæƒ…å†µ
        for execution in self.tracer.executions:
            res_id = execution.resource_id
            if res_id in self.resource_metrics:
                metrics = self.resource_metrics[res_id]
                
                # è®¡ç®—æ‰§è¡Œæ—¶é—´
                duration = execution.end_time - execution.start_time
                metrics.busy_time += duration
                metrics.segment_executions += 1
                
                # ç»Ÿè®¡ä»»åŠ¡æ‰§è¡Œ
                base_task_id = execution.task_id.split('#')[0] if '#' in execution.task_id else execution.task_id
                metrics.task_executions[base_task_id] += 1
        
        # è®¡ç®—åˆ©ç”¨ç‡
        for metrics in self.resource_metrics.values():
            metrics.idle_time = metrics.total_time - metrics.busy_time
            if metrics.total_time > 0:
                metrics.utilization_rate = (metrics.busy_time / metrics.total_time) * 100.0
    
    def _calculate_overall_metrics(self):
        """è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
        # æ‰¾åˆ°å®é™…æ‰§è¡Œç»“æŸæ—¶é—´
        actual_end_time = 0.0
        if self.tracer.executions:
            actual_end_time = max(e.end_time for e in self.tracer.executions)
        
        # è®¡ç®—ç©ºé—²æ—¶é—´
        idle_time = max(0, self.time_window - actual_end_time)
        idle_time_ratio = (idle_time / self.time_window) * 100.0 if self.time_window > 0 else 0
        
        # FPSç»Ÿè®¡
        total_fps_req = sum(m.fps_requirement for m in self.task_metrics.values())
        achieved_fps = sum(m.achieved_fps for m in self.task_metrics.values())
        satisfied_tasks = sum(1 for m in self.task_metrics.values() if m.fps_satisfaction)
        fps_sat_rate = (satisfied_tasks / len(self.task_metrics)) * 100.0 if self.task_metrics else 0
        avg_fps_achievement = sum(m.fps_achievement_rate for m in self.task_metrics.values()) / len(self.task_metrics) if self.task_metrics else 0
        
        # å»¶è¿Ÿç»Ÿè®¡
        all_wait_times = []
        all_latencies = []
        total_violations = 0
        total_instances = 0
        
        for m in self.task_metrics.values():
            all_wait_times.extend(m.wait_times)
            all_latencies.extend(m.latencies)
            total_violations += m.latency_violations
            total_instances += len(m.latencies)
        
        avg_wait = sum(all_wait_times) / len(all_wait_times) if all_wait_times else 0
        avg_latency = sum(all_latencies) / len(all_latencies) if all_latencies else 0
        max_latency = max(all_latencies) if all_latencies else 0
        violation_rate = (total_violations / total_instances) * 100.0 if total_instances > 0 else 0
        
        # èµ„æºåˆ©ç”¨ç‡
        npu_utils = [m.utilization_rate for m in self.resource_metrics.values() 
                     if m.resource_type == ResourceType.NPU]
        dsp_utils = [m.utilization_rate for m in self.resource_metrics.values() 
                     if m.resource_type == ResourceType.DSP]
        
        avg_npu = sum(npu_utils) / len(npu_utils) if npu_utils else 0
        avg_dsp = sum(dsp_utils) / len(dsp_utils) if dsp_utils else 0
        all_utils = list(self.resource_metrics.values())
        overall_util = sum(m.utilization_rate for m in all_utils) / len(all_utils) if all_utils else 0
        
        # èµ„æºè´Ÿè½½å‡è¡¡åº¦ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šå‡è¡¡ï¼‰
        if len(all_utils) > 1:
            utils = [m.utilization_rate for m in all_utils]
            std_dev = np.std(utils)
            # å½’ä¸€åŒ–åˆ°0-1ï¼Œæ ‡å‡†å·®è¶Šå°åˆ†æ•°è¶Šé«˜
            balance_score = max(0, 1 - (std_dev / 50.0))  # å‡è®¾50%æ˜¯æœ€å¤§å¯æ¥å—çš„æ ‡å‡†å·®
        else:
            balance_score = 1.0
        
        # ä»»åŠ¡å®Œæˆç»Ÿè®¡
        total_tasks = len(self.task_metrics)
        completed_tasks = sum(1 for m in self.task_metrics.values() if m.instance_count > 0)
        total_segments = sum(m.execution_count for m in self.task_metrics.values())
        
        # åˆ›å»ºæ•´ä½“æŒ‡æ ‡
        self.overall_metrics = OverallPerformanceMetrics(
            time_window=self.time_window,
            actual_execution_time=actual_end_time,
            idle_time=idle_time,
            idle_time_ratio=idle_time_ratio,
            total_fps_requirement=total_fps_req,
            achieved_total_fps=achieved_fps,
            fps_satisfaction_rate=fps_sat_rate,
            avg_fps_achievement_rate=avg_fps_achievement,
            avg_wait_time=avg_wait,
            avg_latency=avg_latency,
            max_latency=max_latency,
            latency_violation_rate=violation_rate,
            avg_npu_utilization=avg_npu,
            avg_dsp_utilization=avg_dsp,
            overall_resource_utilization=overall_util,
            resource_balance_score=balance_score,
            total_tasks=total_tasks,
            completed_tasks=completed_tasks,
            completion_rate=(completed_tasks / total_tasks) * 100.0 if total_tasks > 0 else 0,
            total_segments=total_segments,
            completed_segments=total_segments  # å‡è®¾æ‰€æœ‰å¼€å§‹çš„æ®µéƒ½å®Œæˆäº†
        )
    
    def print_summary_report(self):
        """æ‰“å°è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
        if not self.overall_metrics:
            print("No evaluation results available")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š æ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
        print("="*80)
        
        # 1. ä»»åŠ¡æ‰§è¡Œè¯¦æƒ…
        print("\n1ï¸âƒ£ ä»»åŠ¡æ‰§è¡Œè¯¦æƒ…:")
        print("-"*80)
        print(f"{'ä»»åŠ¡ID':<15} {'ä¼˜å…ˆçº§':<8} {'FPSè¦æ±‚':<8} {'å®é™…FPS':<8} {'è¾¾æ ‡':<6} "
              f"{'è¿è¡Œæ¬¡æ•°':<8} {'å¹³å‡ç­‰å¾…':<10} {'å¹³å‡å»¶è¿Ÿ':<10} {'å»¶è¿Ÿè¾¾æ ‡ç‡':<10}")
        print("-"*80)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_tasks = sorted(self.task_metrics.values(), 
                            key=lambda m: (m.priority.value, m.task_id))
        
        for m in sorted_tasks:
            fps_ok = "âœ…" if m.fps_satisfaction else "âŒ"
            print(f"{m.task_id:<15} {m.priority.name:<8} {m.fps_requirement:<8.1f} "
                  f"{m.achieved_fps:<8.1f} {fps_ok:<6} {m.instance_count:<8} "
                  f"{m.avg_wait_time:<10.2f} {m.avg_latency:<10.2f} "
                  f"{m.latency_satisfaction_rate:<10.1%}")
        
        # 2. èµ„æºåˆ©ç”¨ç‡
        print("\n2ï¸âƒ£ èµ„æºåˆ©ç”¨ç‡:")
        print("-"*80)
        print(f"{'èµ„æºID':<15} {'ç±»å‹':<8} {'åˆ©ç”¨ç‡':<10} {'å¿™ç¢Œæ—¶é—´':<12} {'ç©ºé—²æ—¶é—´':<12} {'æ‰§è¡Œæ®µæ•°':<10}")
        print("-"*80)
        
        # æŒ‰èµ„æºç±»å‹å’ŒIDæ’åº
        sorted_resources = sorted(self.resource_metrics.values(),
                                key=lambda r: (r.resource_type.value, r.resource_id))
        
        for r in sorted_resources:
            print(f"{r.resource_id:<15} {r.resource_type.value:<8} {r.utilization_rate:<10.1f}% "
                  f"{r.busy_time:<12.1f}ms {r.idle_time:<12.1f}ms {r.segment_executions:<10}")
        
        # 3. æ•´ä½“æ€§èƒ½æŒ‡æ ‡
        m = self.overall_metrics
        print("\n3ï¸âƒ£ æ•´ä½“æ€§èƒ½æŒ‡æ ‡:")
        print("-"*80)
        
        print(f"æ—¶é—´çª—å£: {m.time_window:.1f}ms")
        print(f"å®é™…æ‰§è¡Œæ—¶é—´: {m.actual_execution_time:.1f}ms")
        print(f"ğŸ¯ ç©ºé—²æ—¶é—´: {m.idle_time:.1f}ms ({m.idle_time_ratio:.1f}%)")
        
        print(f"\nFPSæ€§èƒ½:")
        print(f"  - æ€»FPSè¦æ±‚: {m.total_fps_requirement:.1f}")
        print(f"  - å®é™…æ€»FPS: {m.achieved_total_fps:.1f}")
        print(f"  - FPSæ»¡è¶³ç‡: {m.fps_satisfaction_rate:.1f}%")
        print(f"  - å¹³å‡FPSè¾¾æˆç‡: {m.avg_fps_achievement_rate:.1f}%")
        
        print(f"\nå»¶è¿Ÿæ€§èƒ½:")
        print(f"  - å¹³å‡ç­‰å¾…æ—¶é—´: {m.avg_wait_time:.2f}ms")
        print(f"  - å¹³å‡æ€»å»¶è¿Ÿ: {m.avg_latency:.2f}ms")
        print(f"  - æœ€å¤§å»¶è¿Ÿ: {m.max_latency:.2f}ms")
        print(f"  - å»¶è¿Ÿè¿è§„ç‡: {m.latency_violation_rate:.1f}%")
        
        print(f"\nèµ„æºåˆ©ç”¨:")
        print(f"  - NPUå¹³å‡åˆ©ç”¨ç‡: {m.avg_npu_utilization:.1f}%")
        print(f"  - DSPå¹³å‡åˆ©ç”¨ç‡: {m.avg_dsp_utilization:.1f}%")
        print(f"  - æ•´ä½“åˆ©ç”¨ç‡: {m.overall_resource_utilization:.1f}%")
        print(f"  - è´Ÿè½½å‡è¡¡åº¦: {m.resource_balance_score:.2f}")
        
        print(f"\nä»»åŠ¡å®Œæˆ:")
        print(f"  - ä»»åŠ¡å®Œæˆç‡: {m.completion_rate:.1f}% ({m.completed_tasks}/{m.total_tasks})")
        print(f"  - æ€»æ‰§è¡Œæ®µæ•°: {m.total_segments}")
        
        print("\n" + "="*80)
    
    def export_json_report(self, filename: str):
        """å¯¼å‡ºJSONæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        report = {
            "time_window": self.time_window,
            "overall_metrics": self._serialize_overall_metrics(),
            "task_metrics": self._serialize_task_metrics(),
            "resource_metrics": self._serialize_resource_metrics()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
    
    def _serialize_overall_metrics(self) -> Dict:
        """åºåˆ—åŒ–æ•´ä½“æŒ‡æ ‡"""
        if not self.overall_metrics:
            return {}
        
        m = self.overall_metrics
        return {
            "time_window": m.time_window,
            "actual_execution_time": m.actual_execution_time,
            "idle_time": m.idle_time,
            "idle_time_ratio": m.idle_time_ratio,
            "fps": {
                "total_requirement": m.total_fps_requirement,
                "achieved": m.achieved_total_fps,
                "satisfaction_rate": m.fps_satisfaction_rate,
                "avg_achievement_rate": m.avg_fps_achievement_rate
            },
            "latency": {
                "avg_wait_time": m.avg_wait_time,
                "avg_latency": m.avg_latency,
                "max_latency": m.max_latency,
                "violation_rate": m.latency_violation_rate
            },
            "resource_utilization": {
                "npu_avg": m.avg_npu_utilization,
                "dsp_avg": m.avg_dsp_utilization,
                "overall": m.overall_resource_utilization,
                "balance_score": m.resource_balance_score
            },
            "completion": {
                "total_tasks": m.total_tasks,
                "completed_tasks": m.completed_tasks,
                "completion_rate": m.completion_rate,
                "total_segments": m.total_segments
            }
        }
    
    def _serialize_task_metrics(self) -> List[Dict]:
        """åºåˆ—åŒ–ä»»åŠ¡æŒ‡æ ‡"""
        results = []
        for task_id, m in self.task_metrics.items():
            results.append({
                "task_id": m.task_id,
                "task_name": m.task_name,
                "priority": m.priority.name,
                "requirements": {
                    "fps": m.fps_requirement,
                    "latency": m.latency_requirement
                },
                "performance": {
                    "execution_count": m.execution_count,
                    "instance_count": m.instance_count,
                    "achieved_fps": m.achieved_fps,
                    "fps_satisfaction": m.fps_satisfaction,
                    "fps_achievement_rate": m.fps_achievement_rate,
                    "avg_wait_time": m.avg_wait_time,
                    "avg_latency": m.avg_latency,
                    "max_latency": m.max_latency,
                    "latency_violations": m.latency_violations,
                    "latency_satisfaction_rate": m.latency_satisfaction_rate
                }
            })
        return results
    
    def _serialize_resource_metrics(self) -> List[Dict]:
        """åºåˆ—åŒ–èµ„æºæŒ‡æ ‡"""
        results = []
        for res_id, m in self.resource_metrics.items():
            results.append({
                "resource_id": m.resource_id,
                "resource_type": m.resource_type.value,
                "capacity": m.capacity,
                "utilization": {
                    "busy_time": m.busy_time,
                    "idle_time": m.idle_time,
                    "total_time": m.total_time,
                    "utilization_rate": m.utilization_rate
                },
                "executions": {
                    "segment_count": m.segment_executions,
                    "task_breakdown": dict(m.task_executions)
                }
            })
        return results


# å¯¼å…¥numpyç”¨äºè®¡ç®—æ ‡å‡†å·®
try:
    import numpy as np
except ImportError:
    # å¦‚æœæ²¡æœ‰numpyï¼Œæä¾›ç®€å•çš„æ ‡å‡†å·®è®¡ç®—
    def calculate_std(values):
        if not values:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    # æ›¿æ¢numpy.std
    class np:
        @staticmethod
        def std(values):
            return calculate_std(values)
