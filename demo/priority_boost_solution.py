#!/usr/bin/env python3
"""
ä¼˜å…ˆçº§æå‡è§£å†³æ–¹æ¡ˆ - ç›´æ¥è°ƒæ•´å»¶è¿Ÿç´§å¼ ä»»åŠ¡çš„ä¼˜å…ˆçº§
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import Dict, List, Tuple
import copy
from core.resource_queue import ResourceQueueManager
from core.schedule_tracer import ScheduleTracer
from core.launcher import TaskLauncher
from core.executor import ScheduleExecutor
from core.enums import ResourceType, TaskPriority
from core.evaluator import PerformanceEvaluator
from scenario.hybrid_task import create_real_tasks
from viz.schedule_visualizer import ScheduleVisualizer
import numpy as np


def analyze_task_performance(tasks: List, time_window: float = 1000.0) -> Dict[str, Dict]:
    """åˆ†æä»»åŠ¡æ€§èƒ½ï¼Œæ‰¾å‡ºå»¶è¿Ÿç´§å¼ çš„ä»»åŠ¡"""
    print("åˆ†æä»»åŠ¡æ€§èƒ½...")
    
    # åˆ›å»ºèµ„æº
    queue_manager = ResourceQueueManager()
    queue_manager.add_resource("NPU_0", ResourceType.NPU, 40.0)
    queue_manager.add_resource("DSP_0", ResourceType.DSP, 40.0)
    
    tracer = ScheduleTracer(queue_manager)
    launcher = TaskLauncher(queue_manager, tracer)
    
    # æ³¨å†Œä»»åŠ¡
    for task in tasks:
        launcher.register_task(task)
    
    # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
    plan = launcher.create_launch_plan(time_window, "eager")
    executor = ScheduleExecutor(queue_manager, tracer, launcher.tasks)
    executor.segment_mode = True
    executor.execute_plan(plan, time_window)
    
    # è¯„ä¼°æ€§èƒ½
    evaluator = PerformanceEvaluator(tracer, launcher.tasks, queue_manager)
    evaluator.evaluate(time_window, plan.events)
    
    # åˆ†æç»“æœ
    results = {}
    for task_id, metrics in evaluator.task_metrics.items():
        task = launcher.tasks[task_id]
        results[task_id] = {
            'priority': task.priority,
            'fps_requirement': task.fps_requirement,
            'latency_requirement': task.latency_requirement,
            'achieved_fps': metrics.achieved_fps,
            'avg_latency': metrics.avg_latency,
            'max_latency': metrics.max_latency,
            'satisfaction_rate': metrics.latency_satisfaction_rate,
            'violations': len([l for l in metrics.latencies if l > task.latency_requirement]),
            'total_instances': len(metrics.latencies),
            'margin': task.latency_requirement - metrics.max_latency,  # è´Ÿæ•°è¡¨ç¤ºè¶…æ ‡
            'needs_boost': metrics.latency_satisfaction_rate < 0.95  # éœ€è¦æå‡ä¼˜å…ˆçº§
        }
    
    return results


def optimize_task_priorities(tasks: List, performance_data: Dict[str, Dict]) -> List:
    """åŸºäºæ€§èƒ½æ•°æ®ä¼˜åŒ–ä»»åŠ¡ä¼˜å…ˆçº§"""
    print("\nä¼˜åŒ–ä»»åŠ¡ä¼˜å…ˆçº§...")
    
    # å¤åˆ¶ä»»åŠ¡åˆ—è¡¨
    optimized_tasks = copy.deepcopy(tasks)
    
    # æ‰¾å‡ºéœ€è¦æå‡ä¼˜å…ˆçº§çš„ä»»åŠ¡
    boost_candidates = []
    for task_id, data in performance_data.items():
        if data['needs_boost'] and data['priority'] != TaskPriority.CRITICAL:
            boost_candidates.append((task_id, data['margin'], data['satisfaction_rate']))
    
    # æŒ‰å»¶è¿Ÿä½™é‡æ’åºï¼ˆè´Ÿæ•°è¶Šå°è¶Šç´§æ€¥ï¼‰
    boost_candidates.sort(key=lambda x: (x[1], x[2]))
    
    # æå‡ä¼˜å…ˆçº§
    priority_changes = []
    for task_id, margin, satisfaction in boost_candidates:
        for task in optimized_tasks:
            if task.task_id == task_id:
                old_priority = task.priority
                
                # æ ¹æ®ç´§æ€¥ç¨‹åº¦å†³å®šæå‡å¹…åº¦
                if margin < -10:  # ä¸¥é‡è¶…æ ‡
                    if task.priority == TaskPriority.LOW:
                        task.priority = TaskPriority.HIGH
                    elif task.priority == TaskPriority.NORMAL:
                        task.priority = TaskPriority.CRITICAL
                    elif task.priority == TaskPriority.HIGH:
                        task.priority = TaskPriority.CRITICAL
                else:  # è½»å¾®è¶…æ ‡
                    if task.priority == TaskPriority.LOW:
                        task.priority = TaskPriority.NORMAL
                    elif task.priority == TaskPriority.NORMAL:
                        task.priority = TaskPriority.HIGH
                
                if task.priority != old_priority:
                    priority_changes.append({
                        'task_id': task_id,
                        'old': old_priority.name,
                        'new': task.priority.name,
                        'margin': margin,
                        'satisfaction': satisfaction * 100
                    })
                break
    
    # æ‰“å°ä¼˜å…ˆçº§å˜åŒ–
    if priority_changes:
        print("\nä¼˜å…ˆçº§è°ƒæ•´:")
        print("-" * 80)
        print(f"{'ä»»åŠ¡ID':<10} {'åŸä¼˜å…ˆçº§':<12} {'æ–°ä¼˜å…ˆçº§':<12} {'å»¶è¿Ÿä½™é‡':<12} {'æ»¡è¶³ç‡':<10}")
        print("-" * 80)
        for change in priority_changes:
            print(f"{change['task_id']:<10} {change['old']:<12} {change['new']:<12} "
                  f"{change['margin']:<12.1f} {change['satisfaction']:<10.1f}%")
    else:
        print("  æ— éœ€è°ƒæ•´ä¼˜å…ˆçº§")
    
    return optimized_tasks


def evaluate_optimized_solution(optimized_tasks: List, time_window: float = 1000.0):
    """è¯„ä¼°ä¼˜åŒ–åçš„æ–¹æ¡ˆ"""
    print("\nè¯„ä¼°ä¼˜åŒ–æ–¹æ¡ˆ...")
    
    # åˆ›å»ºèµ„æº
    queue_manager = ResourceQueueManager()
    queue_manager.add_resource("NPU_0", ResourceType.NPU, 40.0)
    queue_manager.add_resource("DSP_0", ResourceType.DSP, 40.0)
    
    tracer = ScheduleTracer(queue_manager)
    launcher = TaskLauncher(queue_manager, tracer)
    
    # æ³¨å†Œä¼˜åŒ–åçš„ä»»åŠ¡
    for task in optimized_tasks:
        launcher.register_task(task)
    
    # æ‰§è¡Œè°ƒåº¦
    plan = launcher.create_launch_plan(time_window, "eager")
    executor = ScheduleExecutor(queue_manager, tracer, launcher.tasks)
    executor.segment_mode = True
    
    stats = executor.execute_plan(plan, time_window)
    
    # è¯„ä¼°æ€§èƒ½
    evaluator = PerformanceEvaluator(tracer, launcher.tasks, queue_manager)
    overall_metrics = evaluator.evaluate(time_window, plan.events)
    
    return evaluator, tracer


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 100)
    print("ä¼˜å…ˆçº§æå‡è§£å†³æ–¹æ¡ˆ")
    print("=" * 100)
    
    # 1. åˆ›å»ºä»»åŠ¡
    tasks = create_real_tasks()
    
    # 2. åˆ†æåŸºå‡†æ€§èƒ½
    print("\næ­¥éª¤1: åˆ†æåŸºå‡†æ€§èƒ½")
    print("-" * 100)
    baseline_performance = analyze_task_performance(tasks)
    
    # æ‰“å°é—®é¢˜ä»»åŠ¡
    print("\né—®é¢˜ä»»åŠ¡è¯†åˆ«:")
    problem_tasks = [(tid, data) for tid, data in baseline_performance.items() if data['needs_boost']]
    if problem_tasks:
        for task_id, data in problem_tasks:
            print(f"  {task_id}: æ»¡è¶³ç‡={data['satisfaction_rate']*100:.1f}%, "
                  f"å»¶è¿Ÿä½™é‡={data['margin']:.1f}ms, å½“å‰ä¼˜å…ˆçº§={data['priority'].name}")
    else:
        print("  æ— é—®é¢˜ä»»åŠ¡")
    
    # 3. ä¼˜åŒ–ä¼˜å…ˆçº§
    print("\næ­¥éª¤2: ä¼˜åŒ–ä»»åŠ¡ä¼˜å…ˆçº§")
    print("-" * 100)
    optimized_tasks = optimize_task_priorities(tasks, baseline_performance)
    
    # 4. è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
    print("\næ­¥éª¤3: è¯„ä¼°ä¼˜åŒ–æ•ˆæœ")
    print("-" * 100)
    evaluator, tracer = evaluate_optimized_solution(optimized_tasks)
    
    # 5. å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 100)
    print("ä¼˜åŒ–æ•ˆæœå¯¹æ¯”:")
    print("=" * 100)
    print(f"{'ä»»åŠ¡ID':<10} {'ä»»åŠ¡å':<15} {'åŸä¼˜å…ˆçº§':<12} {'æ–°ä¼˜å…ˆçº§':<12} "
          f"{'åŸºå‡†æ»¡è¶³ç‡':<15} {'ä¼˜åŒ–æ»¡è¶³ç‡':<15} {'æ”¹è¿›':<10}")
    print("-" * 100)
    
    improved_count = 0
    for task_id, baseline_data in baseline_performance.items():
        if task_id in evaluator.task_metrics:
            metrics = evaluator.task_metrics[task_id]
            task = evaluator.tasks[task_id]
            
            baseline_rate = baseline_data['satisfaction_rate'] * 100
            optimized_rate = metrics.latency_satisfaction_rate * 100
            improvement = optimized_rate - baseline_rate
            
            # æ‰¾åˆ°åŸå§‹ä¼˜å…ˆçº§
            original_priority = baseline_data['priority']
            current_priority = task.priority
            
            if improvement > 0:
                improved_count += 1
                status = "âœ“"
            else:
                status = ""
            
            print(f"{task_id:<10} {task.name:<15} {original_priority.name:<12} "
                  f"{current_priority.name:<12} {baseline_rate:<15.1f}% "
                  f"{optimized_rate:<15.1f}% {improvement:+10.1f}% {status}")
    
    print(f"\næ€»ç»“: {improved_count} ä¸ªä»»åŠ¡å¾—åˆ°æ”¹è¿›")
    
    # 6. ç‰¹åˆ«å…³æ³¨T7
    if 'T7' in evaluator.task_metrics:
        print("\nç‰¹åˆ«å…³æ³¨ T7 (tk_search):")
        print(f"  åŸºå‡†æ»¡è¶³ç‡: {baseline_performance['T7']['satisfaction_rate']*100:.1f}%")
        print(f"  ä¼˜åŒ–æ»¡è¶³ç‡: {evaluator.task_metrics['T7'].latency_satisfaction_rate*100:.1f}%")
        print(f"  æ”¹è¿›: {(evaluator.task_metrics['T7'].latency_satisfaction_rate - baseline_performance['T7']['satisfaction_rate'])*100:+.1f}%")
    
    # 7. å¯è§†åŒ–
    visualizer = ScheduleVisualizer(tracer)
    print("\nç”Ÿæˆå¯è§†åŒ–...")
    
    png_filename = "priority_optimized_scheduling.png"
    visualizer.plot_resource_timeline(png_filename)
    print(f"âœ“ ç”Ÿæˆç”˜ç‰¹å›¾: {png_filename}")
    
    print("\n" + "=" * 100)
    print("ğŸ’¡ ç»“è®º:")
    print("=" * 100)
    print("1. é€šè¿‡åˆ†æåŸºå‡†æ€§èƒ½ï¼Œè¯†åˆ«å»¶è¿Ÿç´§å¼ çš„ä»»åŠ¡")
    print("2. æ ¹æ®å»¶è¿Ÿä½™é‡å’Œæ»¡è¶³ç‡ï¼Œæ™ºèƒ½è°ƒæ•´ä»»åŠ¡ä¼˜å…ˆçº§")
    print("3. ä¼˜å…ˆçº§æå‡èƒ½æœ‰æ•ˆæ”¹å–„è¾¹ç•Œä»»åŠ¡çš„å»¶è¿Ÿæ»¡è¶³ç‡")
    print("4. è¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„é™æ€ä¼˜åŒ–æ–¹æ³•")


if __name__ == "__main__":
    main()
