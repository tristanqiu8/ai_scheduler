#!/usr/bin/env python3
"""
çµæ´»çš„è°ƒåº¦æµ‹è¯•æ¡†æ¶ - æ”¯æŒå¤šç§é…ç½®åœºæ™¯
"""

from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import os
import sys

from core.resource_queue import ResourceQueueManager
from core.schedule_tracer import ScheduleTracer
from core.enhanced_launcher import EnhancedTaskLauncher
from core.executor import ScheduleExecutor
from core.evaluator import PerformanceEvaluator
from core.enums import ResourceType
from viz.schedule_visualizer import ScheduleVisualizer

# å¯¼å…¥é…ç½®ç±»
if 'core.scheduling_config' in sys.modules:
    from core.scheduling_config import SchedulingConfig
else:
    from scheduling_config import SchedulingConfig


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    config: 'SchedulingConfig'
    stats: Dict
    metrics: 'PerformanceMetrics'
    utilization: Dict[str, float]
    system_utilization: float
    tracer: ScheduleTracer
    
    def summary(self) -> str:
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        lines = [
            f"é…ç½®: {self.config.get_resource_summary()}",
            f"å®Œæˆå®ä¾‹: {self.stats['completed_instances']}",
            f"Systemåˆ©ç”¨ç‡: {self.system_utilization:.1f}%",
            f"FPSæ»¡è¶³ç‡: {self.metrics.fps_satisfaction_rate:.1f}%"
        ]
        return "\n".join(lines)


class SchedulingTestFramework:
    """è°ƒåº¦æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, tasks: List):
        """
        åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
        """
        self.tasks = tasks
        self.results: Dict[str, TestResult] = {}
    
    def calculate_theory_demand(self, tasks: List, config: SchedulingConfig) -> Dict:
        """
        è®¡ç®—ç†è®ºèµ„æºéœ€æ±‚
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            config: è°ƒåº¦é…ç½®
            
        Returns:
            ç†è®ºéœ€æ±‚åˆ†æç»“æœ
        """
        # è·å–å¹³å‡å¸¦å®½
        npu_bandwidth = config.get_npu_bandwidth()
        dsp_bandwidth = config.get_dsp_bandwidth()
        time_window = config.analysis_window
        
        npu_total_time = 0.0
        dsp_total_time = 0.0
        
        for task in tasks:
            # è®¡ç®—ä»»åŠ¡åœ¨æ—¶é—´çª—å£å†…éœ€è¦æ‰§è¡Œçš„æ¬¡æ•°
            instances_needed = task.fps_requirement * (time_window / 1000.0)
            
            # åº”ç”¨åˆ†æ®µç­–ç•¥è·å–å®é™…æ‰§è¡Œçš„æ®µ
            segments = task.apply_segmentation()
            if not segments:
                segments = task.segments
            
            # è®¡ç®—æ¯ä¸ªæ®µçš„æ‰§è¡Œæ—¶é—´
            for seg in segments:
                if seg.resource_type.value == "NPU":
                    duration = seg.get_duration(npu_bandwidth)
                    npu_total_time += duration * instances_needed
                elif seg.resource_type.value == "DSP":
                    duration = seg.get_duration(dsp_bandwidth)
                    dsp_total_time += duration * instances_needed
        
        # è®¡ç®—åˆ©ç”¨ç‡
        npu_utilization = (npu_total_time / time_window) * 100
        dsp_utilization = (dsp_total_time / time_window) * 100
        
        return {
            'npu_demand_ms': npu_total_time,
            'dsp_demand_ms': dsp_total_time,
            'npu_utilization': npu_utilization,
            'dsp_utilization': dsp_utilization,
            'feasible': npu_utilization <= 100 and dsp_utilization <= 100
        }
    
    def run_test(self, config: SchedulingConfig, verbose: bool = True) -> TestResult:
        """
        è¿è¡Œå•ä¸ªæµ‹è¯•
        
        Args:
            config: è°ƒåº¦é…ç½®
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        if verbose:
            config.print_config()
        
        # 1. åˆ›å»ºèµ„æº
        queue_manager = ResourceQueueManager()
        for resource in config.resources:
            queue_manager.add_resource(
                resource.resource_id,
                resource.resource_type,
                resource.bandwidth
            )
        
        # 2. åˆ›å»ºè¿½è¸ªå™¨å’Œå¯åŠ¨å™¨
        tracer = ScheduleTracer(queue_manager)
        launcher = EnhancedTaskLauncher(queue_manager, tracer)
        
        # 3. æ³¨å†Œä»»åŠ¡
        for task in self.tasks:
            launcher.register_task(task)
        
        # 4. åˆ›å»ºå’Œæ‰§è¡Œè®¡åˆ’
        plan = launcher.create_launch_plan(
            config.simulation_duration,
            config.launch_strategy
        )
        
        executor = ScheduleExecutor(queue_manager, tracer, launcher.tasks)
        stats = executor.execute_plan(
            plan,
            config.simulation_duration,
            segment_mode=config.segment_mode
        )
        
        # 5. è¯„ä¼°æ€§èƒ½
        evaluator = PerformanceEvaluator(tracer, launcher.tasks, queue_manager)
        metrics = evaluator.evaluate(config.simulation_duration, plan.events)
        
        # 6. è®¡ç®—åˆ©ç”¨ç‡
        resource_utilization = tracer.get_resource_utilization(
            time_window=config.analysis_window
        )
        system_util = self._calculate_system_utilization(
            tracer, config.analysis_window
        )
        
        # 7. åˆ›å»ºç»“æœ
        result = TestResult(
            config=config,
            stats=stats,
            metrics=metrics,
            utilization=resource_utilization,
            system_utilization=system_util,
            tracer=tracer
        )
        
        # 8. ä¿å­˜ç»“æœ
        self.results[config.scenario_name] = result
        
        if verbose:
            self._print_test_result(result)
        
        return result
    
    def run_comparison_tests(self, configs: List[SchedulingConfig]) -> Dict[str, TestResult]:
        """
        è¿è¡Œå¤šä¸ªé…ç½®çš„å¯¹æ¯”æµ‹è¯•
        
        Args:
            configs: é…ç½®åˆ—è¡¨
            
        Returns:
            ç»“æœå­—å…¸
        """
        print("\nğŸ”¬ å¼€å§‹å¯¹æ¯”æµ‹è¯•")
        print("="*80)
        
        for config in configs:
            print(f"\nâ–¶ æµ‹è¯•åœºæ™¯: {config.scenario_name}")
            self.run_test(config, verbose=False)
            print(f"  âœ“ å®Œæˆ")
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        self._print_comparison_results()
        
        return self.results
    
    def generate_visualizations(self, output_dir: str = "results"):
        """
        ä¸ºæ‰€æœ‰æµ‹è¯•ç»“æœç”Ÿæˆå¯è§†åŒ–
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶åˆ° {output_dir}/")
        
        for name, result in self.results.items():
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_name = name.replace(" ", "_").replace("Ã—", "x").replace("+", "_")
            
            visualizer = ScheduleVisualizer(result.tracer)
            
            # ç”ŸæˆPNG
            png_file = os.path.join(output_dir, f"{safe_name}.png")
            visualizer.plot_resource_timeline(png_file)
            
            # ç”ŸæˆChrome Trace
            json_file = os.path.join(output_dir, f"{safe_name}.json")
            visualizer.export_chrome_tracing(json_file)
            
            print(f"  âœ“ {name}: {safe_name}.png, {safe_name}.json")
    
    def export_comparison_report(self, filename: str = "comparison_report.txt"):
        """å¯¼å‡ºå¯¹æ¯”æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("è°ƒåº¦ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            # å†™å…¥è¯¦ç»†ç»“æœ
            for name, result in self.results.items():
                f.write(f"\n{name}\n")
                f.write("-"*40 + "\n")
                f.write(result.summary() + "\n")
                
                # èµ„æºåˆ©ç”¨ç‡è¯¦æƒ…
                f.write("\nèµ„æºåˆ©ç”¨ç‡:\n")
                for res_id, util in sorted(result.utilization.items()):
                    f.write(f"  {res_id}: {util:.1f}%\n")
        
        print(f"\nğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    
    def _calculate_system_utilization(self, tracer, window_size):
        """è®¡ç®—ç³»ç»Ÿåˆ©ç”¨ç‡"""
        busy_intervals = []
        
        for exec in tracer.executions:
            if exec.start_time is not None and exec.end_time is not None:
                busy_intervals.append((exec.start_time, exec.end_time))
        
        if not busy_intervals:
            return 0.0
        
        # åˆå¹¶é‡å çš„æ—¶é—´æ®µ
        busy_intervals.sort()
        merged_intervals = []
        
        for start, end in busy_intervals:
            if merged_intervals and start <= merged_intervals[-1][1]:
                merged_intervals[-1] = (merged_intervals[-1][0], max(merged_intervals[-1][1], end))
            else:
                merged_intervals.append((start, end))
        
        total_busy_time = sum(end - start for start, end in merged_intervals)
        return (total_busy_time / window_size) * 100.0
    
    def _print_test_result(self, result: TestResult):
        """æ‰“å°å•ä¸ªæµ‹è¯•ç»“æœ"""
        print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
        print(f"  å®Œæˆå®ä¾‹: {result.stats['completed_instances']}")
        print(f"  æ‰§è¡Œæ®µæ•°: {result.stats['total_segments_executed']}")
        print(f"  Systemåˆ©ç”¨ç‡: {result.system_utilization:.1f}%")
        print(f"  å¹³å‡ç­‰å¾…æ—¶é—´: {result.metrics.avg_wait_time:.2f}ms")
        print(f"  FPSæ»¡è¶³ç‡: {result.metrics.fps_satisfaction_rate*100:.1f}%")
        
        # æ‰“å°ä¸»è¦èµ„æºåˆ©ç”¨ç‡
        npu_utils = [(k, v) for k, v in result.utilization.items() if 'NPU' in k]
        dsp_utils = [(k, v) for k, v in result.utilization.items() if 'DSP' in k]
        
        if npu_utils:
            print(f"\n  NPUåˆ©ç”¨ç‡:")
            for res_id, util in sorted(npu_utils):
                print(f"    {res_id}: {util:.1f}%")
        
        if dsp_utils:
            print(f"\n  DSPåˆ©ç”¨ç‡:")
            for res_id, util in sorted(dsp_utils):
                print(f"    {res_id}: {util:.1f}%")
    
    def _print_comparison_results(self):
        """æ‰“å°å¯¹æ¯”ç»“æœè¡¨æ ¼"""
        if not self.results:
            return
        
        print("\n\nğŸ“Š å¯¹æ¯”ç»“æœæ±‡æ€»")
        print("="*100)
        
        # è¡¨å¤´
        headers = ["é…ç½®", "å®Œæˆå®ä¾‹", "Systemåˆ©ç”¨ç‡", "å¹³å‡NPUåˆ©ç”¨ç‡", "å¹³å‡DSPåˆ©ç”¨ç‡", "FPSæ»¡è¶³ç‡"]
        col_widths = [25, 10, 15, 15, 15, 12]
        
        # æ‰“å°è¡¨å¤´
        header_line = ""
        for header, width in zip(headers, col_widths):
            header_line += f"{header:<{width}}"
        print(header_line)
        print("-"*100)
        
        # æ‰“å°æ•°æ®è¡Œ
        for name, result in self.results.items():
            # è®¡ç®—å¹³å‡åˆ©ç”¨ç‡
            npu_utils = [v for k, v in result.utilization.items() if 'NPU' in k]
            dsp_utils = [v for k, v in result.utilization.items() if 'DSP' in k]
            
            avg_npu = sum(npu_utils) / len(npu_utils) if npu_utils else 0
            avg_dsp = sum(dsp_utils) / len(dsp_utils) if dsp_utils else 0
            
            row = [
                name[:24],  # æˆªæ–­è¿‡é•¿çš„åç§°
                str(result.stats['completed_instances']),
                f"{result.system_utilization:.1f}%",
                f"{avg_npu:.1f}%",
                f"{avg_dsp:.1f}%",
                f"{result.metrics.fps_satisfaction_rate:.1f}%"
            ]
            
            row_line = ""
            for cell, width in zip(row, col_widths):
                row_line += f"{cell:<{width}}"
            print(row_line)
